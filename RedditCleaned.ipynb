{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be56d7e-6cf7-4620-a1a9-ed643c2d2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reddit Post Sentiment Analysis - Economy-Related Posts from Midwest States\n",
    "===========================================================================\n",
    "\n",
    "This script scrapes Reddit posts about the economy from five Midwest states,\n",
    "performs sentiment analysis on the post titles, and exports the results to CSV.\n",
    "\n",
    "Author: [Your Name]\n",
    "Date: December 2024\n",
    "Python Version: 3.12+\n",
    "\n",
    "Dependencies:\n",
    "- praw (Reddit API wrapper)\n",
    "- pandas (data manipulation)\n",
    "- nltk (sentiment analysis)\n",
    "\n",
    "Setup Instructions:\n",
    "1. Install required packages: pip install praw pandas nltk\n",
    "2. Create a Reddit app at https://www.reddit.com/prefs/apps\n",
    "3. Replace the client_id and client_secret with your credentials\n",
    "4. Run the notebook cells in order\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74a328-bf8e-4f76-aaf8-3b33f2b2a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Install PRAW (Python Reddit API Wrapper)\n",
    "# Uncomment and run this if you need to install praw\n",
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e39995-ab40-4e61-9d6f-e06282719e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Import Required Libraries and Scrape Reddit Posts\n",
    "\n",
    "import time\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Reddit API client\n",
    "# IMPORTANT: Replace these credentials with your own from https://www.reddit.com/prefs/apps\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"YOUR_CLIENT_ID_HERE\",          # Replace with your client ID\n",
    "    client_secret=\"YOUR_CLIENT_SECRET_HERE\",   # Replace with your client secret\n",
    "    user_agent=\"economy_sentiment_scraper by u/YOUR_USERNAME\"  # Replace with your username\n",
    ")\n",
    "\n",
    "def scrape_economy_posts_by_state(limit=500, requests_per_minute=50):\n",
    "    \"\"\"\n",
    "    Scrape Reddit posts about the economy for specified Midwest states.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    limit : int, default=500\n",
    "        Maximum number of posts to retrieve per state\n",
    "    requests_per_minute : int, default=50\n",
    "        Rate limit for API requests (Reddit allows 60/minute for most apps)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Creates global DataFrames for each state (Ohio, Wisconsin, Indiana, Illinois, Michigan)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the states to analyze\n",
    "    states = [\"Ohio\", \"Wisconsin\", \"Indiana\", \"Illinois\", \"Michigan\"]\n",
    "    \n",
    "    # Define date range: 2015-01-01 to 2025-12-31\n",
    "    # Convert to Unix timestamps (seconds since 1970-01-01)\n",
    "    start_range = int(datetime(2015, 1, 1).timestamp())\n",
    "    end_range = int(datetime(2025, 12, 31, 23, 59, 59).timestamp())\n",
    "    \n",
    "    # Loop through each state\n",
    "    for state in states:\n",
    "        print(f\"Scraping posts for {state}...\")\n",
    "        posts = []\n",
    "        request_count = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create search query: \"economy\" + state name\n",
    "        search_query = f\"economy {state}\"\n",
    "        \n",
    "        # Search all of Reddit for posts matching the query\n",
    "        for submission in reddit.subreddit(\"all\").search(search_query, limit=limit):\n",
    "            \n",
    "            # Only keep posts created between 2015 and 2025\n",
    "            if start_range <= submission.created_utc <= end_range:\n",
    "                posts.append({\n",
    "                    \"state\": state,\n",
    "                    \"title\": submission.title,\n",
    "                    \"score\": submission.score,  # Reddit upvote score\n",
    "                    \"url\": submission.url,\n",
    "                    \"created_utc\": submission.created_utc,  # Unix timestamp\n",
    "                    \"date\": datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"selftext\": submission.selftext  # Body text of post (if any)\n",
    "                })\n",
    "            \n",
    "            # Rate limit protection: pause after reaching request limit\n",
    "            request_count += 1\n",
    "            if request_count >= requests_per_minute:\n",
    "                elapsed = time.time() - start_time\n",
    "                if elapsed < 60:\n",
    "                    sleep_time = 60 - elapsed\n",
    "                    print(f\"Rate limit reached. Sleeping {sleep_time:.2f} seconds...\")\n",
    "                    time.sleep(sleep_time)\n",
    "                request_count = 0\n",
    "                start_time = time.time()\n",
    "        \n",
    "        # Create a global DataFrame for this state\n",
    "        globals()[state] = pd.DataFrame(posts)\n",
    "        print(f\"Collected {len(posts)} posts for {state}.\\n\")\n",
    "\n",
    "# Run the scraping function\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_economy_posts_by_state(limit=500)\n",
    "    \n",
    "    # Display sample of Ohio data\n",
    "    print(\"Ohio sample:\")\n",
    "    print(Ohio[['title', 'date', 'score']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342f75f-b330-41d3-a7d5-7c903249873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Perform Sentiment Analysis on Post Titles\n",
    "\n",
    "from nltk import download\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download VADER lexicon (only needed once)\n",
    "# VADER = Valence Aware Dictionary and sEntiment Reasoner\n",
    "# Specifically tuned for social media text\n",
    "download('vader_lexicon')\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# List of state DataFrames to process\n",
    "states = [\"Ohio\", \"Wisconsin\", \"Indiana\", \"Illinois\", \"Michigan\"]\n",
    "\n",
    "# Loop through each state and add sentiment scores\n",
    "for state in states:\n",
    "    print(f\"Adding sentiment scores to {state}...\")\n",
    "    \n",
    "    # Get the post titles from the DataFrame\n",
    "    corpus = globals()[state]['title']\n",
    "    \n",
    "    # Analyze sentiment for each title\n",
    "    sentiment_results = []\n",
    "    for sentence in corpus:\n",
    "        # polarity_scores returns a dictionary with:\n",
    "        # - 'neg': negative sentiment (0-1)\n",
    "        # - 'neu': neutral sentiment (0-1)\n",
    "        # - 'pos': positive sentiment (0-1)\n",
    "        # - 'compound': overall sentiment (-1 to +1)\n",
    "        ss = sia.polarity_scores(sentence)\n",
    "        sentiment_results.append(ss)\n",
    "    \n",
    "    # Convert sentiment results to DataFrame\n",
    "    sentiment_df = pd.DataFrame(sentiment_results)\n",
    "    \n",
    "    # Add sentiment columns to the existing state DataFrame\n",
    "    globals()[state] = pd.concat(\n",
    "        [globals()[state].reset_index(drop=True), sentiment_df.reset_index(drop=True)],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Display sample with sentiment scores\n",
    "print(\"Ohio sample with sentiment:\")\n",
    "print(Ohio[['title', 'neg', 'neu', 'pos', 'compound']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6e931-3d33-49e6-9731-47110ad50e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Export Data to CSV Files\n",
    "\n",
    "# Save each state's data to a separate CSV file\n",
    "Ohio.to_csv(\"Ohio_2015-2025.csv\", index=False)\n",
    "Wisconsin.to_csv(\"Wisconsin_2015-2025.csv\", index=False)\n",
    "Indiana.to_csv(\"Indiana_2015-2025.csv\", index=False)\n",
    "Michigan.to_csv(\"Michigan_2015-2025.csv\", index=False)\n",
    "Illinois.to_csv(\"Illinois_2015-2025.csv\", index=False)\n",
    "\n",
    "print(\"All data exported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b993b-0a2a-45f6-bad9-d7d853cc9a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: Create Summary Statistics Table (Optional)\n",
    "\n",
    "# This section demonstrates how to create a formatted summary table\n",
    "# showing sentiment trends by year\n",
    "\n",
    "# Read back one of the CSV files (example with Illinois)\n",
    "df = pd.read_csv(\"Illinois_2015-2025.csv\")\n",
    "\n",
    "# Convert date column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Define sentiment columns to analyze\n",
    "sentiment_cols = ['compound', 'neg', 'neu', 'pos']\n",
    "\n",
    "# Group by year and calculate mean values\n",
    "summary_by_year = (\n",
    "    df.groupby('year')[sentiment_cols]\n",
    "      .agg(['mean'])\n",
    "      .round(4)\n",
    ")\n",
    "\n",
    "# Flatten column names for cleaner display\n",
    "summary_by_year.columns = [f\"{col}_{stat}\" for col, stat in summary_by_year.columns]\n",
    "\n",
    "# Create styled table with color gradient\n",
    "styled_table = (\n",
    "    summary_by_year.style\n",
    "        .set_caption(\"Sentiment Summary Statistics by Year\")\n",
    "        .set_table_styles([\n",
    "            {\"selector\": \"caption\", \n",
    "             \"props\": [(\"font-size\", \"16px\"), \n",
    "                      (\"font-weight\", \"bold\"), \n",
    "                      (\"text-align\", \"center\"), \n",
    "                      (\"padding\", \"10px\")]},\n",
    "            {\"selector\": \"th\", \n",
    "             \"props\": [(\"background-color\", \"#e8e8e8\"), \n",
    "                      (\"font-weight\", \"bold\"), \n",
    "                      (\"border\", \"1px solid #bfbfbf\")]},\n",
    "            {\"selector\": \"td\", \n",
    "             \"props\": [(\"border\", \"1px solid #d9d9d9\"), \n",
    "                      (\"padding\", \"6px\")]}\n",
    "        ])\n",
    "        .background_gradient(cmap=\"Blues\", axis=0)\n",
    "        .highlight_max(axis=0, color=\"#a3d0ff\")\n",
    "        .highlight_min(axis=0, color=\"#ffd6cc\")\n",
    ")\n",
    "\n",
    "# Display the styled table\n",
    "styled_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46882526-3c8e-4dbe-8702-3c7af5bdd10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OUTPUT FILES:\n",
    "-------------\n",
    "- Ohio_2015-2025.csv\n",
    "- Wisconsin_2015-2025.csv\n",
    "- Indiana_2015-2025.csv\n",
    "- Illinois_2015-2025.csv\n",
    "- Michigan_2015-2025.csv\n",
    "\n",
    "COLUMNS IN OUTPUT:\n",
    "------------------\n",
    "- state: State name\n",
    "- title: Reddit post title\n",
    "- score: Reddit upvote score\n",
    "- url: Post URL\n",
    "- created_utc: Unix timestamp\n",
    "- date: Human-readable date\n",
    "- num_comments: Number of comments\n",
    "- selftext: Post body text\n",
    "- neg: Negative sentiment score (0-1)\n",
    "- neu: Neutral sentiment score (0-1)\n",
    "- pos: Positive sentiment score (0-1)\n",
    "- compound: Overall sentiment score (-1 to +1)\n",
    "\n",
    "INTERPRETING SENTIMENT SCORES:\n",
    "-------------------------------\n",
    "Compound Score:\n",
    "  - Positive sentiment: compound > 0.05\n",
    "  - Neutral sentiment: -0.05 <= compound <= 0.05\n",
    "  - Negative sentiment: compound < -0.05\n",
    "  \n",
    "The neg, neu, and pos scores always sum to 1.0 and represent \n",
    "the proportion of text that falls into each category.\n",
    "\n",
    "NOTES:\n",
    "------\n",
    "- Reddit API has rate limits (typically 60 requests/minute)\n",
    "- The script includes automatic rate limiting and pauses\n",
    "- Sentiment analysis is performed on titles only (not post bodies)\n",
    "- VADER is optimized for social media text and handles emojis, \n",
    "  capitalization, and punctuation\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
